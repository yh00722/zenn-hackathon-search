"""
RAG (Retrieval Augmented Generation) ã‚µãƒ¼ãƒ“ã‚¹
==============================================
ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ChromaDBã€åŸ‹ã‚è¾¼ã¿ã¨ãƒãƒ£ãƒƒãƒˆã«OpenAI/Azure OpenAIã‚’ä½¿ç”¨
"""
from pathlib import Path
from typing import Optional

from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from .config import settings
from .database import db
from .llm_factory import get_chat_llm, get_embeddings, check_llm_available


# æ—¥æœ¬èªRAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
RAG_PROMPT_JA = PromptTemplate(
    template="""ã‚ãªãŸã¯Zenn AI Agent Hackathonã®ä½œå“ã«è©³ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ¤œç´¢ã•ã‚ŒãŸé–¢é€£è¨˜äº‹ï¼‰ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

å›ç­”ï¼ˆå…·ä½“çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ™ã’ãªãŒã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ï¼‰:""",
    input_variables=["context", "question"]
)


class RAGService:
    """ãƒãƒƒã‚«ã‚½ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¯ã‚¨ãƒªç”¨RAGã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        settings.ensure_dirs()
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not check_llm_available():
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        # åŸ‹ã‚è¾¼ã¿ã®åˆæœŸåŒ–ï¼ˆAzure/OpenAI è‡ªå‹•é¸æŠï¼‰
        self.embeddings = get_embeddings()
        
        # LLMã®åˆæœŸåŒ–ï¼ˆAzure/OpenAI è‡ªå‹•é¸æŠï¼‰
        self.llm = get_chat_llm()
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–ï¼ˆcontent_raw ç”¨ï¼‰
        self.vectorstore = Chroma(
            persist_directory=str(settings.CHROMA_DB_PATH),
            embedding_function=self.embeddings,
            collection_name="hackathon_projects"
        )
        
        # 2æ®µéšæ¤œç´¢ç”¨: content_summary ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
        self.summary_vectorstore = Chroma(
            persist_directory=str(settings.CHROMA_DB_PATH) + "_summary",
            embedding_function=self.embeddings,
            collection_name="project_summaries"
        )
        
        # retriever ã¯ summary ã‚’ä½¿ç”¨ã€k=8 ã«å¤‰æ›´
        self.retriever = self.summary_vectorstore.as_retriever(search_kwargs={"k": 8})
    
    def index_projects(self, edition: Optional[int] = None) -> int:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        Args:
            edition: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒãƒƒã‚«ã‚½ãƒ³å›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        """
        print("ğŸ”„ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ChromaDBã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­...")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        projects = db.get_projects(edition=edition, limit=1000)
        
        # é•·ã„è¨˜äº‹ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", " "]
        )
        
        documents = []
        for project in projects:
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            content = project.get("content_raw") or project.get("description") or ""
            if not content or len(content) < 50:
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            metadata = {
                "project_id": project["id"],
                "project_name": project["project_name"],
                "url": project["url"],
                "edition": project["hackathon_id"],
                "author_name": project["author_name"],
                "likes": project["likes"],
                "is_winner": bool(project["is_winner"]),
                "award_name": project.get("award_name") or "",
            }
            
            # é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†å‰²
            chunks = text_splitter.split_text(content)
            
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={**metadata, "chunk_index": i}
                )
                documents.append(doc)
        
        if documents:
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
            self.vectorstore.add_documents(documents)
            print(f"âœ… {len(projects)}ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰{len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã—ã¾ã—ãŸ")
        else:
            print("âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        
        return len(documents)
    
    def index_summaries(self, edition: Optional[int] = None) -> int:
        """
        content_summary ã‚’åˆ¥ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ2æ®µéšæ¤œç´¢ç”¨ï¼‰
        
        Args:
            edition: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒãƒƒã‚«ã‚½ãƒ³å›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        """
        print("ğŸ”„ è¦ç´„ã‚’ChromaDBã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­...")
        
        projects = db.get_projects(edition=edition, limit=1000)
        
        documents = []
        for project in projects:
            summary = project.get("content_summary") or ""
            if not summary or len(summary) < 20:
                continue
            
            metadata = {
                "project_id": project["id"],
                "project_name": project["project_name"],
                "url": project["url"],
                "edition": project["hackathon_id"],
                "author_name": project["author_name"],
                "likes": project["likes"],
                "bookmarks": project.get("bookmarks", 0),
                "is_winner": bool(project["is_winner"]),
                "award_name": project.get("award_name") or "",
            }
            
            doc = Document(
                page_content=summary,
                metadata=metadata
            )
            documents.append(doc)
        
        if documents:
            self.summary_vectorstore.add_documents(documents)
            print(f"âœ… {len(documents)}ä»¶ã®è¦ç´„ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã—ã¾ã—ãŸ")
        else:
            print("âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹è¦ç´„ãŒã‚ã‚Šã¾ã›ã‚“")
        
        return len(documents)
    
    def query(self, question: str) -> dict:
        """
        2æ®µéšæ¤œç´¢ã§RAGã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        
        Stage 1: content_summary ã§ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆç²¾åº¦é‡è¦–ï¼‰
        Stage 2: content_raw ã‚’å–å¾—ã—ã¦LLMã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä½¿ç”¨ï¼ˆæƒ…å ±é‡é‡è¦–ï¼‰
        
        Args:
            question: æ—¥æœ¬èªã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
        
        Returns:
            å›ç­”ã¨ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å«ã‚€è¾æ›¸
        """
        # Stage 1: summary ã§é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œç´¢
        summary_docs = self.retriever.invoke(question)
        
        # Stage 2: é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® content_raw ã‚’å–å¾—
        project_ids = [doc.metadata.get("project_id") for doc in summary_docs]
        
        # content_raw ã‚’å–å¾—
        full_contents = []
        with db.get_connection() as conn:
            for pid in project_ids:
                if pid:
                    row = conn.execute(
                        "SELECT project_name, content_raw FROM projects WHERE id = ?",
                        (pid,)
                    ).fetchone()
                    if row and row[1]:
                        # é•·ã™ãã‚‹å ´åˆã¯ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
                        content = row[1][:2000] if len(row[1]) > 2000 else row[1]
                        full_contents.append(f"## {row[0]}\n\n{content}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆcontent_raw ã‹ã‚‰ï¼‰
        if full_contents:
            context = "\n\n---\n\n".join(full_contents)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: summary ã‚’ä½¿ç”¨
            context = "\n\n---\n\n".join([doc.page_content for doc in summary_docs])
        
        docs = summary_docs  # sources ç”¨ã«ä¿æŒ
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        prompt = RAG_PROMPT_JA.format(context=context, question=question)
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        response = self.llm.invoke(prompt)
        
        sources = []
        for doc in docs:
            sources.append({
                "project_name": doc.metadata.get("project_name"),
                "url": doc.metadata.get("url"),
                "edition": doc.metadata.get("edition"),
                "is_winner": doc.metadata.get("is_winner"),
            })
        
        # URLã§é‡è¤‡é™¤å»
        seen_urls = set()
        unique_sources = []
        for src in sources:
            if src["url"] not in seen_urls:
                seen_urls.add(src["url"])
                unique_sources.append(src)
        
        return {
            "answer": response.content,
            "sources": unique_sources
        }
    
    def similarity_search(self, query: str, k: int = 5) -> list[dict]:
        """
        LLMç”Ÿæˆãªã—ã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: çµæœæ•°
        
        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã®ãƒãƒƒãƒã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        docs = self.vectorstore.similarity_search(query, k=k)
        
        results = []
        for doc in docs:
            results.append({
                "content": doc.page_content[:500],
                "metadata": doc.metadata
            })
        
        return results


# é…å»¶åˆæœŸåŒ–
_rag_service: Optional[RAGService] = None

def get_rag_service() -> RAGService:
    """RAGã‚µãƒ¼ãƒ“ã‚¹ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã®å–å¾—ã¾ãŸã¯ä½œæˆ"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service
