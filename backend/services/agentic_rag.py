"""
Agentic RAG with LangGraph
==========================
ãƒžãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ»ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«ã‚ˆã‚‹é«˜åº¦ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
"""
from typing import Annotated, TypedDict, Literal
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from .config import settings
from .database import db
from .llm_factory import get_chat_llm


class AgentState(TypedDict):
    """Agent ã®çŠ¶æ…‹ï¼ˆLangGraph æŽ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    question: str                                      # å…ƒã®è³ªå•
    messages: Annotated[list, add_messages]            # ä¼šè©±å±¥æ­´ï¼ˆreducer ã§è‡ªå‹•è¿½åŠ ï¼‰
    iteration: int                                     # ç¾åœ¨ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
    final_answer: str | None                           # æœ€çµ‚å›žç­”


# ============================================================
# ãƒ„ãƒ¼ãƒ«å®šç¾©
# ============================================================

@tool
def semantic_search_summary(query: str, k: int = 4) -> list[dict]:
    """
    ã€æŽ¨å¥¨ã€‘ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå˜ä½ã®ä¸»é¡Œæ¤œç´¢
    
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã€‡ã€‡é–¢é€£ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã€Œâ–³â–³ã‚’ä½¿ã£ãŸä½œå“ã€ãªã©
    ä¸»é¡Œãƒ»ãƒ†ãƒ¼ãƒžã§æ¤œç´¢ã—ãŸã„å ´åˆã«ä½¿ç”¨ã€‚
    
    å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¦ç´„ï¼ˆContentSummaryï¼‰ã‹ã‚‰æ¤œç´¢ã—ã€
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå˜ä½ã§1ä»¶ãšã¤è¿”ã™ãŸã‚é‡è¤‡ãªãé–¢é€£ä½œå“ã‚’è¦‹ã¤ã‘ã‚„ã™ã„ã€‚
    """
    from .rag import get_rag_service
    rag = get_rag_service()
    docs = rag.summary_vectorstore.similarity_search(query, k=k)
    return [{"name": d.metadata.get("project_name"), 
             "url": d.metadata.get("url"),
             "summary": d.page_content,
             "is_winner": d.metadata.get("is_winner", False),
             "award_name": d.metadata.get("award_name"),
             "award_comment": d.metadata.get("award_comment")} for d in docs]


@tool
def semantic_search_content(query: str, k: int = 5) -> list[dict]:
    """
    è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã®è©³ç´°ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆæ¤œç´¢
    
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã€‡ã€‡ã®å®Ÿè£…æ–¹æ³•ã€ã€Œå…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã€ã€Œè©³ç´°ãªæ‰‹é †ã€ãªã©
    è¨˜äº‹å†…ã®ç‰¹å®šæ®µè½ãƒ»æŠ€è¡“å®Ÿè£…ã®è©³ç´°ã‚’æŽ¢ã—ãŸã„å ´åˆã«ä½¿ç”¨ã€‚
    
    è¨˜äº‹æœ¬æ–‡ï¼ˆcontent_rawï¼‰ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ¤œç´¢ã™ã‚‹ãŸã‚ã€
    å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚„å®Ÿè£…è©³ç´°ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«é©ã—ã¦ã„ã‚‹ã€‚
    """
    from .rag import get_rag_service
    rag = get_rag_service()
    docs = rag.vectorstore.similarity_search(query, k=k)
    
    seen = set()
    results = []
    for d in docs:
        name = d.metadata.get("project_name")
        if name not in seen:
            seen.add(name)
            results.append({
                "name": name,
                "url": d.metadata.get("url"),
                "content_excerpt": d.page_content,
                "is_winner": d.metadata.get("is_winner", False),
                "award_name": d.metadata.get("award_name"),
                "award_comment": d.metadata.get("award_comment"),
            })
    return results


@tool
def text2sql_query(query: str) -> dict:
    """
    SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆãƒ»å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
    
    ä»¥ä¸‹ã®æƒ…å ±ã‚’å–å¾—ã—ãŸã„å ´åˆã«ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼š
    - å—è³žä½œå“ä¸€è¦§ã€è³žã®åå‰ï¼ˆaward_nameï¼‰ã€å¯©æŸ»å“¡ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆaward_commentï¼‰
    - ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã„ã„ã­æ•°é †ã€ãƒ–ãƒƒã‚¯ãƒžãƒ¼ã‚¯æ•°é †ï¼‰
    - çµ±è¨ˆæƒ…å ±ï¼ˆä»¶æ•°ã€é›†è¨ˆï¼‰
    - ãƒãƒ¼ãƒ /å€‹äººã§ã®çµžã‚Šè¾¼ã¿
    - å›žåˆ¥ï¼ˆç¬¬1å›ž/ç¬¬2å›ž/ç¬¬3å›žï¼‰ã§ã®çµžã‚Šè¾¼ã¿
    - æœ€çµ‚é¸è€ƒé€²å‡ºä½œå“ï¼ˆis_final_pitchï¼‰
    """
    from .text2sql import Text2SQLGenerator
    gen = Text2SQLGenerator()
    return gen.execute(query)


@tool
def get_project_detail(project_name: str) -> dict:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    with db.get_connection() as conn:
        row = conn.execute(
            "SELECT * FROM projects WHERE project_name LIKE ? LIMIT 1",
            (f"%{project_name}%",)
        ).fetchone()
        return dict(row) if row else {"error": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}


@tool
def keyword_search(keyword: str) -> list[dict]:
    """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§LIKEæ¤œç´¢ï¼ˆæŠ€è¡“åãªã©å®Œå…¨ä¸€è‡´å‘ã‘ï¼‰"""
    with db.get_connection() as conn:
        rows = conn.execute(
            """SELECT project_name, url, description 
               FROM projects WHERE content_raw LIKE ? LIMIT 10""",
            (f"%{keyword}%",)
        ).fetchall()
        return [dict(r) for r in rows]


TOOLS = [semantic_search_summary, semantic_search_content, text2sql_query, get_project_detail, keyword_search]


# ============================================================
# LangGraph Agent
# ============================================================

SYSTEM_PROMPT = """ã‚ãªãŸã¯Zenn AI Agent Hackathonã®ä½œå“æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦æƒ…å ±ã‚’åŽé›†ã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
- semantic_search_summary: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¦ç´„ã‹ã‚‰æ„å‘³æ¤œç´¢ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæŽ¢ç´¢å‘ã‘ï¼‰
- semantic_search_content: è¨˜äº‹æœ¬æ–‡ã‹ã‚‰è©³ç´°ã‚’æ¤œç´¢ï¼ˆæŠ€è¡“å®Ÿè£…å‘ã‘ï¼‰
- text2sql_query: ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚„çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰
- get_project_detail: ç‰¹å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©³ç´°ã‚’å–å¾—
- keyword_search: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å…¨æ–‡æ¤œç´¢ï¼ˆæŠ€è¡“åæ¤œç´¢å‘ã‘ï¼‰

ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸã‚‰ã€ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã°ãšã«å›žç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""


class AgenticRAG:
    """LangGraph ãƒ™ãƒ¼ã‚¹ã® Agentic RAG"""
    
    def __init__(self, max_iterations: int = 3):
        self.max_iterations = max_iterations
        
        # LLMã®åˆæœŸåŒ–ï¼ˆAzure/OpenAI è‡ªå‹•é¸æŠžï¼‰
        self.llm = get_chat_llm()
        
        # ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰
        self.llm_with_tools = self.llm.bind_tools(TOOLS)
        
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """ReActãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
        workflow = StateGraph(AgentState)
        
        # ãƒŽãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("agent", self._agent_node)
        workflow.add_node("tools", ToolNode(TOOLS))
        workflow.add_node("increment_iteration", self._increment_iteration)
        workflow.add_node("generate", self._generate_node)
        
        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        workflow.set_entry_point("agent")
        
        # æ¡ä»¶åˆ†å²
        workflow.add_conditional_edges(
            "agent",
            self._should_continue,
            {
                "continue": "tools",
                "generate": "generate",
            }
        )
        
        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œ â†’ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¢—åŠ  â†’ Agent ã«æˆ»ã‚‹
        workflow.add_edge("tools", "increment_iteration")
        workflow.add_edge("increment_iteration", "agent")
        
        # ç”Ÿæˆå¾Œ â†’ çµ‚äº†
        workflow.add_edge("generate", END)
        
        return workflow.compile()
    
    def _agent_node(self, state: AgentState) -> dict:
        """Agent ãƒŽãƒ¼ãƒ‰: ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã¶ã‹åˆ¤æ–­"""
        messages = state["messages"]
        
        # æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        if len(messages) == 1:
            messages = [SystemMessage(content=SYSTEM_PROMPT)] + list(messages)
        
        response = self.llm_with_tools.invoke(messages)
        return {"messages": [response]}
    
    def _increment_iteration(self, state: AgentState) -> dict:
        """ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’å¢—åŠ """
        return {"iteration": state["iteration"] + 1}
    
    def _should_continue(self, state: AgentState) -> Literal["continue", "generate"]:
        """æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ±ºå®š"""
        last_message = state["messages"][-1]
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸Šé™ãƒã‚§ãƒƒã‚¯
            if state["iteration"] >= self.max_iterations:
                return "generate"
            return "continue"
        
        return "generate"
    
    def _generate_node(self, state: AgentState) -> dict:
        """æœ€çµ‚å›žç­”ã‚’ç”Ÿæˆ"""
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‹ã‚‰æ¤œç´¢çµæžœã‚’æŠ½å‡º
        tool_results = []
        for msg in state["messages"]:
            if isinstance(msg, ToolMessage):
                tool_results.append(f"[{msg.name}]: {msg.content}")
        
        prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæžœã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªžã§å›žç­”ã—ã¦ãã ã•ã„ã€‚

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{state["question"]}

## æ¤œç´¢çµæžœ
{chr(10).join(tool_results) if tool_results else "æ¤œç´¢çµæžœãªã—"}

## å›žç­”ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- å…·ä½“çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ™ã’ãªãŒã‚‰èª¬æ˜Ž
- å—è³žä½œå“ã¯ðŸ†ãƒžãƒ¼ã‚¯ã§å¼·èª¿
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æ­£ç›´ã«ä¼ãˆã‚‹

å›žç­”:"""
        
        response = self.llm.invoke(prompt)
        return {"final_answer": response.content}
    
    def query(self, question: str) -> dict:
        """è³ªå•ã«å›žç­”"""
        initial_state = {
            "question": question,
            "messages": [HumanMessage(content=question)],
            "iteration": 0,
            "final_answer": None
        }
        
        result = self.graph.invoke(initial_state)
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›žæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        tool_call_count = sum(
            1 for m in result.get("messages", [])
            if isinstance(m, AIMessage) and hasattr(m, "tool_calls") and m.tool_calls
        )
        
        return {
            "answer": result.get("final_answer", "å›žç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ"),
            "iterations": result.get("iteration", 0),
            "tool_calls": tool_call_count
        }
    
    def query_stream(self, question: str):
        """
        è³ªå•ã«å›žç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆï¼‰
        
        Yields:
            (event_type, data) ã‚¿ãƒ—ãƒ«:
            - "metadata": åˆæœŸæƒ…å ±ï¼ˆæˆ¦ç•¥ã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰
            - "token": å›žç­”ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨
            - "done": å®Œäº†ã‚·ã‚°ãƒŠãƒ«
            - "error": ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            initial_state = {
                "question": question,
                "messages": [HumanMessage(content=question)],
                "iteration": 0,
                "final_answer": None
            }
            
            # ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œï¼ˆãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—éƒ¨åˆ†ï¼‰
            # generate ãƒŽãƒ¼ãƒ‰ã‚’é™¤ã„ãŸçŠ¶æ…‹ã¾ã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
            # LangGraph ã® compile() å¾Œã®ã‚°ãƒ©ãƒ•ã¯é€”ä¸­åœæ­¢ãŒé›£ã—ã„ãŸã‚ã€
            # ä»£ã‚ã‚Šã«ãƒ•ãƒ«ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œã—ã¦æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å†æ§‹ç¯‰ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            
            result = self.graph.invoke(initial_state)
            
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›žæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            tool_call_count = sum(
                1 for m in result.get("messages", [])
                if isinstance(m, AIMessage) and hasattr(m, "tool_calls") and m.tool_calls
            )
            
            # ãƒ„ãƒ¼ãƒ«çµæžœã‚’æŠ½å‡º
            tool_results = []
            sources = []
            for msg in result.get("messages", []):
                if isinstance(msg, ToolMessage):
                    tool_results.append(f"[{msg.name}]: {msg.content}")
                    # ã‚½ãƒ¼ã‚¹URLã‚’æŠ½å‡º
                    try:
                        import json
                        data = json.loads(msg.content) if isinstance(msg.content, str) else msg.content
                        if isinstance(data, list):
                            for item in data:
                                if isinstance(item, dict) and item.get("url"):
                                    sources.append({
                                        "project_name": item.get("name") or item.get("project_name"),
                                        "url": item.get("url"),
                                        "is_winner": item.get("is_winner", False)
                                    })
                        elif isinstance(data, dict) and data.get("results"):
                            for item in data["results"]:
                                if isinstance(item, dict) and item.get("url"):
                                    sources.append({
                                        "project_name": item.get("project_name"),
                                        "url": item.get("url"),
                                        "is_winner": bool(item.get("is_winner"))
                                    })
                    except:
                        pass
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’yield
            yield ("metadata", {
                "strategy": "agentic_rag",
                "explanation": f"iterations={result.get('iteration', 0)}, tool_calls={tool_call_count}",
                "sources": sources[:10]
            })
            
            # æœ€çµ‚å›žç­”ãŒã™ã§ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆéžã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ç”Ÿæˆæ¸ˆã¿ï¼‰
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã«å†ç”Ÿæˆ
            prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæžœã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªžã§å›žç­”ã—ã¦ãã ã•ã„ã€‚

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{question}

## æ¤œç´¢çµæžœ
{chr(10).join(tool_results) if tool_results else "æ¤œç´¢çµæžœãªã—"}

## å›žç­”ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- å…·ä½“çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ™ã’ãªãŒã‚‰èª¬æ˜Ž
- å—è³žä½œå“ã¯ðŸ†ãƒžãƒ¼ã‚¯ã§å¼·èª¿
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æ­£ç›´ã«ä¼ãˆã‚‹

å›žç­”:"""
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›žç­”ã‚’ç”Ÿæˆ
            for chunk in self.llm.stream(prompt):
                if chunk.content:
                    yield ("token", chunk.content)
            
            yield ("done", None)
            
        except Exception as e:
            yield ("error", str(e))


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼ˆmax_iterations ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
_agentic_rag = None

def get_agentic_rag(max_iterations: int = 3) -> AgenticRAG:
    global _agentic_rag
    if _agentic_rag is None or _agentic_rag.max_iterations != max_iterations:
        _agentic_rag = AgenticRAG(max_iterations=max_iterations)
    return _agentic_rag
